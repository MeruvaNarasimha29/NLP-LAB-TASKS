{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeoeYLXXefw2Ct+9voCM6I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JKUAL8Gorbg","executionInfo":{"status":"ok","timestamp":1758783083422,"user_tz":-330,"elapsed":642,"user":{"displayName":"TUMMEPALLI PAVANKARTHIK,CSE(2022) Vel Tech, Chennai","userId":"12502267669211204278"}},"outputId":"f8ddb1be-6b9d-4727-843d-46a7896e84ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Nouns: ['Web', 'example', 'web', 'page', 'text', 'parts', 'speech', 'example', 'cat', 'dog', 'verb', 'prepositions']\n","Verbs: ['contains', 'jumps', 'contains']\n","Adjectives: ['various', 'lazy']\n","Entities: []\n"]}],"source":["from bs4 import BeautifulSoup\n","import spacy\n","\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def pos_tag_and_extract_info(text):\n","    doc = nlp(text)\n","    nouns = []\n","    verbs = []\n","    adjectives = []\n","    entities = []\n","\n","    for token in doc:\n","        if token.pos_ == \"NOUN\":\n","            nouns.append(token.text)\n","        elif token.pos_ == \"VERB\":\n","            verbs.append(token.text)\n","        elif token.pos_ == \"ADJ\":\n","            adjectives.append(token.text)\n","\n","    for entity in doc.ents:\n","        entities.append((entity.text, entity.label_))\n","\n","    return nouns, verbs, adjectives, entities\n","\n","\n","web_document = \"\"\"\n","<html>\n","<head>\n","<title>Example Web Page</title>\n","</head>\n","<body>\n","<p>This is an example web page. It contains some text with various parts of speech.</p>\n","<p>For example, \"The cat jumps over the lazy dog\" contains a noun, a verb, and prepositions.</p>\n","</body>\n","</html>\n","\"\"\"\n","\n","def extract_text_from_html(html):\n","    soup = BeautifulSoup(html, 'html.parser')\n","    return soup.get_text()\n","\n","text_content = extract_text_from_html(web_document)\n","nouns, verbs, adjectives, entities = pos_tag_and_extract_info(text_content)\n","\n","print(\"Nouns:\", nouns)\n","print(\"Verbs:\", verbs)\n","print(\"Adjectives:\", adjectives)\n","print(\"Entities:\", entities)"]}]}